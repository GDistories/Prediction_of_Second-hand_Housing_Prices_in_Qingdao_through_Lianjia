{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-10T08:13:41.976357Z",
     "start_time": "2023-10-10T08:04:00.780908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/ershoufang/shinan/', '/ershoufang/shibei/', '/ershoufang/licang/', '/ershoufang/laoshan/', '/ershoufang/huangdao/', '/ershoufang/chengyang/', '/ershoufang/jiaozhou/', '/ershoufang/jimo/', '/ershoufang/pingdu/', '/ershoufang/laixi/']\n",
      "Scraping area: /ershoufang/shinan/\n",
      "Current area: /ershoufang/shinan/. 10 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 20 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 30 entries scraped.\n",
      "Scraping for area /ershoufang/shinan/ finished. 30 entries in total.\n",
      "Scraping area: /ershoufang/shibei/\n",
      "Current area: /ershoufang/shibei/. 10 entries scraped.\n",
      "Current area: /ershoufang/shibei/. 20 entries scraped.\n",
      "Current area: /ershoufang/shibei/. 30 entries scraped.\n",
      "Scraping for area /ershoufang/shibei/ finished. 30 entries in total.\n",
      "Scraping area: /ershoufang/licang/\n",
      "Current area: /ershoufang/licang/. 10 entries scraped.\n",
      "Current area: /ershoufang/licang/. 20 entries scraped.\n",
      "Current area: /ershoufang/licang/. 30 entries scraped.\n",
      "Scraping for area /ershoufang/licang/ finished. 30 entries in total.\n",
      "Scraping area: /ershoufang/laoshan/\n",
      "Current area: /ershoufang/laoshan/. 10 entries scraped.\n",
      "Current area: /ershoufang/laoshan/. 20 entries scraped.\n",
      "Current area: /ershoufang/laoshan/. 30 entries scraped.\n",
      "Scraping for area /ershoufang/laoshan/ finished. 30 entries in total.\n",
      "Scraping area: /ershoufang/huangdao/\n",
      "Current area: /ershoufang/huangdao/. 10 entries scraped.\n",
      "Current area: /ershoufang/huangdao/. 20 entries scraped.\n",
      "Current area: /ershoufang/huangdao/. 30 entries scraped.\n",
      "Scraping for area /ershoufang/huangdao/ finished. 30 entries in total.\n",
      "Scraping area: /ershoufang/chengyang/\n",
      "Current area: /ershoufang/chengyang/. 10 entries scraped.\n",
      "Current area: /ershoufang/chengyang/. 20 entries scraped.\n",
      "Current area: /ershoufang/chengyang/. 30 entries scraped.\n",
      "Scraping for area /ershoufang/chengyang/ finished. 30 entries in total.\n",
      "Scraping area: /ershoufang/jiaozhou/\n",
      "Current area: /ershoufang/jiaozhou/. 10 entries scraped.\n",
      "Current area: /ershoufang/jiaozhou/. 20 entries scraped.\n",
      "Current area: /ershoufang/jiaozhou/. 30 entries scraped.\n",
      "Scraping for area /ershoufang/jiaozhou/ finished. 30 entries in total.\n",
      "Scraping area: /ershoufang/jimo/\n",
      "Current area: /ershoufang/jimo/. 10 entries scraped.\n",
      "Current area: /ershoufang/jimo/. 20 entries scraped.\n",
      "Current area: /ershoufang/jimo/. 30 entries scraped.\n",
      "Scraping for area /ershoufang/jimo/ finished. 30 entries in total.\n",
      "Scraping area: /ershoufang/pingdu/\n",
      "No second-hand houses in this area.\n",
      "Scraping area: /ershoufang/laixi/\n",
      "No second-hand houses in this area.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "\n",
    "# Set fixed part of list page URL\n",
    "base_url = 'http://qd.lianjia.com'\n",
    "\n",
    "# Set variable part of the page\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'}\n",
    "\n",
    "# Loop through to grab list page info\n",
    "# Get area\n",
    "district_url = base_url + '/ershoufang'\n",
    "district_response = requests.get(url=district_url, headers=headers)\n",
    "district_html = district_response.content\n",
    "district_encoding = district_response.encoding\n",
    "district_parsed = etree.HTML(district_html, parser=etree.HTMLParser(encoding=district_encoding))\n",
    "areas_links = district_parsed.xpath('//div[@class=\"position\"]/dl[2]/dd/div[1]/div/a/@href')\n",
    "\n",
    "print(areas_links)\n",
    "\n",
    "for area_link in areas_links:\n",
    "    print('Scraping area:', area_link)\n",
    "    start_time = time.time() \n",
    "    areas = []\n",
    "    prices = []\n",
    "    house_details = []\n",
    "    follow_details = []\n",
    "    price_per_unit = []\n",
    "    features = []\n",
    "    transaction_details = []\n",
    "    basic_details = []\n",
    "\n",
    "    current_area_count = 0\n",
    "    full_url = base_url + area_link\n",
    "    area_response = requests.get(url=full_url, headers=headers)\n",
    "    area_html = area_response.content\n",
    "    area_encoding = area_response.encoding\n",
    "    area_parsed = etree.HTML(area_html, parser=etree.HTMLParser(encoding=area_encoding))\n",
    "    page_data_results = area_parsed.xpath(\"//div[@class='contentBottom clear']/div[@class='page-box fr']//@page-data\")\n",
    "    \n",
    "    if page_data_results:\n",
    "        total_pages = eval(page_data_results[0])['totalPage']\n",
    "    else:\n",
    "        print(\"No second-hand houses in this area.\")\n",
    "        continue\n",
    "\n",
    "    for i in range(1, 2):\n",
    "        try:\n",
    "            page_url = base_url + area_link + 'pg' + str(i) + '/'\n",
    "            response = requests.get(url=page_url, headers=headers)\n",
    "            html_content = response.content\n",
    "            time.sleep(1)\n",
    "            encoding = response.encoding\n",
    "            parsed_content = etree.HTML(html_content, parser=etree.HTMLParser(encoding=encoding))\n",
    "            \n",
    "            # Extract total price of houses\n",
    "            for item in parsed_content.xpath('//div[@class=\"priceInfo\"]'):\n",
    "                total_price = item.xpath('.//span/text()')[0]\n",
    "                prices.append(total_price)\n",
    "\n",
    "            # Extract price per unit\n",
    "            for item in parsed_content.xpath('//div[@class=\"unitPrice\"]'):\n",
    "                unit_price = item.xpath('.//span/text()')[0]\n",
    "                price_per_unit.append(unit_price)\n",
    "\n",
    "            # Extract house details\n",
    "            for item in parsed_content.xpath('//div[@class=\"info clear\"]'):\n",
    "                feature_tags = item.xpath('.//div[@class=\"tag\"]')\n",
    "                for tag in feature_tags:\n",
    "                    feature = tag.xpath('.//span/text()')\n",
    "                    features.append(feature)\n",
    "                house_detail = item.xpath('.//div[@class=\"positionInfo\"]//a/text()')[0] + '|' + item.xpath('.//div[@class=\"positionInfo\"]//a[2]/text()')[0] + '|' + item.xpath('.//div[@class=\"houseInfo\"]//text()')[0]\n",
    "                house_details.append(house_detail)\n",
    "\n",
    "            # Extract basic and transaction details\n",
    "            house_urls = parsed_content.xpath('//div[@class=\"info clear\"]//div[@class=\"title\"]/a/@href')\n",
    "            for house_url in house_urls:\n",
    "                house_response = requests.get(url=house_url, headers=headers)\n",
    "                house_html = house_response.content\n",
    "                time.sleep(1)\n",
    "                house_encoding = house_response.encoding\n",
    "                house_parsed = etree.HTML(house_html, parser=etree.HTMLParser(encoding=house_encoding))\n",
    "                transaction_data = house_parsed.xpath('//div[@class=\"transaction\"]//div[@class=\"content\"]//ul//li//span[2]//text()')\n",
    "                basic_data = house_parsed.xpath('//div[@class=\"base\"]//div[@class=\"content\"]//ul//li/text()')\n",
    "                transaction_details.append(transaction_data)\n",
    "                basic_details.append(basic_data)\n",
    "                areas.append(area_link)\n",
    "                current_area_count += 1\n",
    "                \n",
    "                if current_area_count % 10 == 0:\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print(f\"Current area: {area_link}. {current_area_count} entries scraped. Elapsed time: {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "            # Extract follow details\n",
    "            for item in parsed_content.xpath('//div[@class=\"followInfo\"]'):\n",
    "                follow_detail = item.xpath('./text()')[0]\n",
    "                follow_details.append(follow_detail)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error on page {i} for area {area_link}: {str(e)}\")\n",
    "    \n",
    "    # Save to a DataFrame and then to CSV for every area\n",
    "    house_df = pd.DataFrame({\n",
    "            'area': areas,\n",
    "            'house_details': house_details,\n",
    "            'follow_details': follow_details,\n",
    "            'prices': prices,\n",
    "            'price_per_unit': price_per_unit,\n",
    "            'features': features,\n",
    "            'basic_details': basic_details,\n",
    "            'transaction_details': transaction_details\n",
    "        })\n",
    "    house_df.to_csv(\"Raw_data.csv\", mode='a', header=False, encoding='utf-8', index=False)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Scraping for area {area_link} finished. {current_area_count} entries in total.\")\n",
    "    print(f\"Scraping for area {area_link} finished. {current_area_count} entries in total. Total time taken: {elapsed_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3305245c86916805"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
