{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-10T08:02:05.401439Z",
     "start_time": "2023-10-10T07:39:38.555173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/ershoufang/shinan/', '/ershoufang/shibei/', '/ershoufang/licang/', '/ershoufang/laoshan/', '/ershoufang/huangdao/', '/ershoufang/chengyang/', '/ershoufang/jiaozhou/', '/ershoufang/jimo/', '/ershoufang/pingdu/', '/ershoufang/laixi/']\n",
      "Scraping area: /ershoufang/shinan/\n",
      "Current area: /ershoufang/shinan/. 10 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 20 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 30 entries scraped.\n",
      "Scraping area: /ershoufang/shinan/\n",
      "Current area: /ershoufang/shinan/. 40 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 50 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 60 entries scraped.\n",
      "Scraping area: /ershoufang/shinan/\n",
      "Current area: /ershoufang/shinan/. 70 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 80 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 90 entries scraped.\n",
      "Scraping area: /ershoufang/shinan/\n",
      "Current area: /ershoufang/shinan/. 100 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 110 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 120 entries scraped.\n",
      "Scraping area: /ershoufang/shinan/\n",
      "Current area: /ershoufang/shinan/. 130 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 140 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 150 entries scraped.\n",
      "Scraping area: /ershoufang/shinan/\n",
      "Current area: /ershoufang/shinan/. 160 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 170 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 180 entries scraped.\n",
      "Scraping area: /ershoufang/shinan/\n",
      "Current area: /ershoufang/shinan/. 190 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 200 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 210 entries scraped.\n",
      "Scraping area: /ershoufang/shinan/\n",
      "Current area: /ershoufang/shinan/. 220 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 230 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 240 entries scraped.\n",
      "Scraping area: /ershoufang/shinan/\n",
      "Current area: /ershoufang/shinan/. 250 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 260 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 270 entries scraped.\n",
      "Scraping area: /ershoufang/shinan/\n",
      "Current area: /ershoufang/shinan/. 280 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 290 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 300 entries scraped.\n",
      "Scraping area: /ershoufang/shinan/\n",
      "Current area: /ershoufang/shinan/. 310 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 320 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 330 entries scraped.\n",
      "Scraping area: /ershoufang/shinan/\n",
      "Current area: /ershoufang/shinan/. 340 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 350 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 360 entries scraped.\n",
      "Scraping area: /ershoufang/shinan/\n",
      "Current area: /ershoufang/shinan/. 370 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 380 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 390 entries scraped.\n",
      "Scraping area: /ershoufang/shinan/\n",
      "Current area: /ershoufang/shinan/. 400 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 410 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 420 entries scraped.\n",
      "Scraping area: /ershoufang/shinan/\n",
      "Current area: /ershoufang/shinan/. 430 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 440 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 450 entries scraped.\n",
      "Scraping area: /ershoufang/shinan/\n",
      "Current area: /ershoufang/shinan/. 460 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 470 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 480 entries scraped.\n",
      "Scraping area: /ershoufang/shinan/\n",
      "Current area: /ershoufang/shinan/. 490 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 500 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 510 entries scraped.\n",
      "Scraping area: /ershoufang/shinan/\n",
      "Current area: /ershoufang/shinan/. 520 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 530 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 540 entries scraped.\n",
      "Scraping area: /ershoufang/shinan/\n",
      "Current area: /ershoufang/shinan/. 550 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 560 entries scraped.\n",
      "Current area: /ershoufang/shinan/. 570 entries scraped.\n",
      "Scraping area: /ershoufang/shinan/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 54\u001B[0m\n\u001B[1;32m     52\u001B[0m html_content \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mcontent\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mScraping area:\u001B[39m\u001B[38;5;124m'\u001B[39m, area_link)\n\u001B[0;32m---> 54\u001B[0m time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     55\u001B[0m encoding \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mencoding\n\u001B[1;32m     56\u001B[0m parsed_content \u001B[38;5;241m=\u001B[39m etree\u001B[38;5;241m.\u001B[39mHTML(html_content, parser\u001B[38;5;241m=\u001B[39metree\u001B[38;5;241m.\u001B[39mHTMLParser(encoding\u001B[38;5;241m=\u001B[39mencoding))\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "\n",
    "# Set fixed part of list page URL\n",
    "base_url = 'http://qd.lianjia.com'\n",
    "\n",
    "# Set variable part of the page\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'}\n",
    "\n",
    "# Loop through to grab list page info\n",
    "# Get area\n",
    "district_url = base_url + '/ershoufang'\n",
    "district_response = requests.get(url=district_url, headers=headers)\n",
    "district_html = district_response.content\n",
    "district_encoding = district_response.encoding\n",
    "district_parsed = etree.HTML(district_html, parser=etree.HTMLParser(encoding=district_encoding))\n",
    "areas_links = district_parsed.xpath('//div[@class=\"position\"]/dl[2]/dd/div[1]/div/a/@href')\n",
    "\n",
    "print(areas_links)\n",
    "\n",
    "for area_link in areas_links:\n",
    "    print('Scraping area:', area_link)\n",
    "    areas = []\n",
    "    prices = []\n",
    "    house_details = []\n",
    "    follow_details = []\n",
    "    price_per_unit = []\n",
    "    features = []\n",
    "    transaction_details = []\n",
    "    basic_details = []\n",
    "\n",
    "    current_area_count = 0\n",
    "    full_url = base_url + area_link\n",
    "    area_response = requests.get(url=full_url, headers=headers)\n",
    "    area_html = area_response.content\n",
    "    area_encoding = area_response.encoding\n",
    "    area_parsed = etree.HTML(area_html, parser=etree.HTMLParser(encoding=area_encoding))\n",
    "    page_data_results = area_parsed.xpath(\"//div[@class='contentBottom clear']/div[@class='page-box fr']//@page-data\")\n",
    "    \n",
    "    if page_data_results:\n",
    "        total_pages = eval(page_data_results[0])['totalPage']\n",
    "    else:\n",
    "        print(\"No second-hand houses in this area.\")\n",
    "        continue\n",
    "\n",
    "    for i in range(1, total_pages+1):\n",
    "        try:\n",
    "            page_url = base_url + area_link + 'pg' + str(i) + '/'\n",
    "            response = requests.get(url=page_url, headers=headers)\n",
    "            html_content = response.content\n",
    "            time.sleep(1)\n",
    "            encoding = response.encoding\n",
    "            parsed_content = etree.HTML(html_content, parser=etree.HTMLParser(encoding=encoding))\n",
    "            \n",
    "            # Extract total price of houses\n",
    "            for item in parsed_content.xpath('//div[@class=\"priceInfo\"]'):\n",
    "                total_price = item.xpath('.//span/text()')[0]\n",
    "                prices.append(total_price)\n",
    "\n",
    "            # Extract price per unit\n",
    "            for item in parsed_content.xpath('//div[@class=\"unitPrice\"]'):\n",
    "                unit_price = item.xpath('.//span/text()')[0]\n",
    "                price_per_unit.append(unit_price)\n",
    "\n",
    "            # Extract house details\n",
    "            for item in parsed_content.xpath('//div[@class=\"info clear\"]'):\n",
    "                feature_tags = item.xpath('.//div[@class=\"tag\"]')\n",
    "                for tag in feature_tags:\n",
    "                    feature = tag.xpath('.//span/text()')\n",
    "                    features.append(feature)\n",
    "                house_detail = item.xpath('.//div[@class=\"positionInfo\"]//a/text()')[0] + '|' + item.xpath('.//div[@class=\"positionInfo\"]//a[2]/text()')[0] + '|' + item.xpath('.//div[@class=\"houseInfo\"]//text()')[0]\n",
    "                house_details.append(house_detail)\n",
    "\n",
    "            # Extract basic and transaction details\n",
    "            house_urls = parsed_content.xpath('//div[@class=\"info clear\"]//div[@class=\"title\"]/a/@href')\n",
    "            for house_url in house_urls:\n",
    "                house_response = requests.get(url=house_url, headers=headers)\n",
    "                house_html = house_response.content\n",
    "                time.sleep(1)\n",
    "                house_encoding = house_response.encoding\n",
    "                house_parsed = etree.HTML(house_html, parser=etree.HTMLParser(encoding=house_encoding))\n",
    "                transaction_data = house_parsed.xpath('//div[@class=\"transaction\"]//div[@class=\"content\"]//ul//li//span[2]//text()')\n",
    "                basic_data = house_parsed.xpath('//div[@class=\"base\"]//div[@class=\"content\"]//ul//li/text()')\n",
    "                transaction_details.append(transaction_data)\n",
    "                basic_details.append(basic_data)\n",
    "                areas.append(area_link)\n",
    "                current_area_count += 1\n",
    "                \n",
    "                if current_area_count % 10 == 0:\n",
    "                    print(f\"Current area: {area_link}. {current_area_count} entries scraped.\")\n",
    "\n",
    "            # Extract follow details\n",
    "            for item in parsed_content.xpath('//div[@class=\"followInfo\"]'):\n",
    "                follow_detail = item.xpath('./text()')[0]\n",
    "                follow_details.append(follow_detail)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error on page {i} for area {area_link}: {str(e)}\")\n",
    "    \n",
    "    # Save to a DataFrame and then to CSV for every area\n",
    "    house_df = pd.DataFrame({\n",
    "            'area': areas,\n",
    "            'house_details': house_details,\n",
    "            'follow_details': follow_details,\n",
    "            'prices': prices,\n",
    "            'price_per_unit': price_per_unit,\n",
    "            'features': features,\n",
    "            'basic_details': basic_details,\n",
    "            'transaction_details': transaction_details\n",
    "        })\n",
    "    house_df.to_csv(\"Raw_data.csv\", mode='a', header=False, encoding='utf-8', index=False)\n",
    "    print(f\"Scraping for area {area_link} finished. {current_area_count} entries in total.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3305245c86916805"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
